{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Data Types and Sizes\n",
        "It is imoportant to understand data types and their sizes. As quantization is basically converting one data type to another."
      ],
      "metadata": {
        "id": "XwjIhBLsnXIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "oA-IovMcnhxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integers**: In the following notice the range of different data types."
      ],
      "metadata": {
        "id": "sD9peVUzpLsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Information of `8-bit unsigned integer`\")\n",
        "print(torch.iinfo(torch.uint8))\n",
        "print(\"Information of `8-bit (signed) integer`\")\n",
        "print(torch.iinfo(torch.int8))\n",
        "print(\"Information of `64-bit (signed) integer`\")\n",
        "print(torch.iinfo(torch.int64))\n",
        "print(\"Information of `32-bit (signed) integer`\")\n",
        "print(torch.iinfo(torch.int32))\n",
        "print(\"Information of `16-bit (signed) integer`\")\n",
        "print(torch.iinfo(torch.int16))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiC9gxu5ohV0",
        "outputId": "d33af8e2-c713-48cb-9148-dd982d9f101d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information of `8-bit unsigned integer`\n",
            "iinfo(min=0, max=255, dtype=uint8)\n",
            "Information of `8-bit (signed) integer`\n",
            "iinfo(min=-128, max=127, dtype=int8)\n",
            "Information of `64-bit (signed) integer`\n",
            "iinfo(min=-9.22337e+18, max=9.22337e+18, dtype=int64)\n",
            "Information of `32-bit (signed) integer`\n",
            "iinfo(min=-2.14748e+09, max=2.14748e+09, dtype=int32)\n",
            "Information of `16-bit (signed) integer`\n",
            "iinfo(min=-32768, max=32767, dtype=int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Floats**"
      ],
      "metadata": {
        "id": "pBKK4x4XppAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# by default, python stores float data in fp64\n",
        "value = 1/3"
      ],
      "metadata": {
        "id": "tkiVV_pMpJcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format(value, '.60f')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "c6axTOJmpzBj",
        "outputId": "51277986-46e6-4588-cafb-0a472ec9b357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.333333333333333314829616256247390992939472198486328125000000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 64-bit floating point\n",
        "tensor_fp64 = torch.tensor(value, dtype = torch.float64)\n",
        "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t10f6pOSp1Pz",
        "outputId": "22a6a7c4-55a9-495c-a234-e4882a1480f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_fp32 = torch.tensor(value, dtype = torch.float32)\n",
        "tensor_fp16 = torch.tensor(value, dtype = torch.float16)\n",
        "tensor_bf16 = torch.tensor(value, dtype = torch.bfloat16)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")\n",
        "print(f\"fp32 tensor: {format(tensor_fp32.item(), '.60f')}\")\n",
        "print(f\"fp16 tensor: {format(tensor_fp16.item(), '.60f')}\")\n",
        "print(f\"bf16 tensor: {format(tensor_bf16.item(), '.60f')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPMBau-Ip5KB",
        "outputId": "dbd110c2-7661-4da8-f2a4-c6569eb9467b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\n",
            "fp32 tensor: 0.333333343267440795898437500000000000000000000000000000000000\n",
            "fp16 tensor: 0.333251953125000000000000000000000000000000000000000000000000\n",
            "bf16 tensor: 0.333984375000000000000000000000000000000000000000000000000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Information of `16-bit brain floating point`\")\n",
        "print(torch.finfo(torch.bfloat16))\n",
        "\n",
        "print(\"Information of `32-bit floating point`\")\n",
        "print(torch.finfo(torch.float32))\n",
        "\n",
        "\n",
        "print(\"Information of `16-bit floating point`\")\n",
        "print(torch.finfo(torch.float16))\n",
        "\n",
        "\n",
        "print(\"Information of `64-bit floating point`\")\n",
        "print(torch.finfo(torch.float64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSmC-fwSp7xY",
        "outputId": "85279f50-6fb6-4ab2-b5cf-8bd2e4771064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information of `16-bit brain floating point`\n",
            "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n",
            "Information of `32-bit floating point`\n",
            "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\n",
            "Information of `16-bit floating point`\n",
            "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\n",
            "Information of `64-bit floating point`\n",
            "finfo(resolution=1e-15, min=-1.79769e+308, max=1.79769e+308, eps=2.22045e-16, smallest_normal=2.22507e-308, tiny=2.22507e-308, dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downcasting**"
      ],
      "metadata": {
        "id": "vZ0XgR_QsYcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_fp32 = torch.rand(1000, dtype = torch.float32)\n",
        "tensor_fp32[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kto-tNYnsVsr",
        "outputId": "e559c98b-735f-48a5-89f6-0897e24950a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3667, 0.7698, 0.8428, 0.2310, 0.1133])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_fp32_to_bf16 = tensor_fp32.to(dtype = torch.bfloat16)\n",
        "tensor_fp32_to_bf16[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax0q-zX3sg7d",
        "outputId": "04bcf0a2-0cba-4618-80a4-fc84ee9092fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3672, 0.7695, 0.8438, 0.2314, 0.1133], dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_float32 = torch.dot(tensor_fp32, tensor_fp32)\n",
        "m_float32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bw3Dyvcsl39",
        "outputId": "0b5be5ea-e626-418d-925d-f59a63beef5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(336.8615)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_bfloat16 = torch.dot(tensor_fp32_to_bf16, tensor_fp32_to_bf16)\n",
        "m_bfloat16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIHcWfEWsq3M",
        "outputId": "cbdab224-6323-4ebc-a138-0d2542ba8090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(336., dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "\n",
        "*   In this notebook we studied the data types interger and floats and their sizes.\n",
        "*   And the efect of downsizing the data ( typecasting) to have smaller space.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UZtE6F-9oLbs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xlIDf66Bs1Fc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}